Here we provide analytical tools to test effectiveness of the representations learned
by vision models when pretrained on datasets like ImageNet-1k. These tools are
from [1]. The code mostly comes from [this repository](https://github.com/sayakpaul/robustness-vit).

Each directory provides a standalone `README.md` that contains further instructions
on how each notebook / script should be run.

## References

[1] Paul, Sayak, and Pin-Yu Chen. Vision Transformers Are Robust Learners. AAAI 2022, https://doi.org/10.48550/arXiv.2105.07581.